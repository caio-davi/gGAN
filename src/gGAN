#!/usr/bin/env python

import os
import sys
from sys import exit
import argparse
import shutil
from datetime import datetime

LOG_PATH = '/scratch/user/caio.davi/Workspace/gGAN/' 
PATH = '/home/caio.davi/Workspace/gGAN/'

def check_afd(afd):
    enabled_models = ['0.07', '0.10', '0.21', '1.00','SVM']
    if afd in enabled_models:
        print("[INFO] Running GAN with a max allelic frequency proximity of:", afd)
    else:
        print("[ERROR] Invalid model set:", afd)
        exit()
    return afd.replace(".", "")

def make_model_path(net_model):
    model_name = str(net_model).split()[1]
    # path to save logs, performances and fake samples files
    path = LOG_PATH + 'run/'
    if not os.path.exists(path):
        os.makedirs(path)
    path = path + 'test_'+model_name+'_'+datetime.now().isoformat()
    os.mkdir(path)
    return path

def load_datasets(path, afd, dim, syn, log_path, random=True):
    import load_data
    import pre_processing
    if(syn):
        if(not pre_processing.check_current_sampling(afd)):
            print("[ERROR] Current synthetic data doesn't match [afd] option.", dim)
            exit()
        print('-------------ENTRA AQUI -------------------------------------')
        print("[INFO] Loading synthetic labeled data...")    
        labeled_dataset = load_data.load_labeled_samples(path + 'data/synthetic/labeled')
    else:
        print("[INFO] Pre-Processing Data...")
        print("[INFO] Dimensions: ", dim)
        pre_processing.init(path, afd, dim)

        print("[INFO] Loading original labeled data...")
        labeled_dataset = load_data.load_labeled_samples(path + 'data/labeled')
    
    print("[INFO] Loading original unlabeled data...")
    unlabeled_dataset = load_data.load_unlabeled_samples(path + 'data/unlabeled')
    # generate train and test LABELED datasets
    labeled_train_dataset, labeled_test_dataset = load_data.generate_supervised_datasets(labeled_dataset, log_path, random)
    # generate train and test UNLABELED datasets
    unlabeled_train_dataset, unlabeled_test_dataset = load_data.generate_unsupervised_datasets(unlabeled_dataset, log_path, random)
    datasets = {
        'labeled_train_dataset' : labeled_train_dataset,
        'labeled_test_dataset' : labeled_test_dataset,
        'unlabeled_train_dataset' : unlabeled_train_dataset,
        'unlabeled_test_dataset' : unlabeled_test_dataset,
        'unlabeled_dataset' : unlabeled_dataset,
    }
    return datasets

def main():
    # parse args
    parser = argparse.ArgumentParser()
    parser.add_argument("cmd", help="Action to perform. Opitons: run : run, clear, test.")
    parser.add_argument("--afd", help="The threshold for the Allelic Freqeuncy Distance. Options are: 0.07, 0.10, 0.21, SVM")
#    parser.add_argument("--dim", help="Number of dimensions of the formated sample. Options are: 1 (Conv1D) or 2 (Conv2D)")
    parser.add_argument("--syn", type=bool, help="Run training+test with synthetic data.")
    args = parser.parse_args()
    args.dim = '1'
    enabled_dims = [1,2]
    path = PATH

    if not (float(args.dim) in enabled_dims):
        print("[ERROR] Invalid Dimension option:", args.dim)
        exit()

    if args.cmd == 'clear':
        print("[INFO] Deleting all previus results...")
        shutil.rmtree(path + "run/")
        exit()

    elif args.cmd == 'test':
        import model
        from keras.models import load_model
        afd = check_afd(args.afd)
        backup_path = path + 'backups/'+afd
        c_model = load_model(backup_path+'/c_model.h5')
        d_model = load_model(backup_path+'/d_model.h5', compile=True)
        g_model = load_model(backup_path+'/g_model.h5', compile=True)
        datasets = load_datasets(path, afd, args.dim, args.syn, backup_path, False)
        model.tests(g_model, d_model, c_model, 100, datasets['labeled_test_dataset'], datasets['unlabeled_test_dataset'])

    elif args.cmd == 'run':
        import model
        afd = check_afd(args.afd)
        sys.path.insert(1, path +'src/models/')
        if(str(afd) == 'SVM'):
            net_model = __import__('model_'+args.dim+'_007', globals(), locals(), 0)
        else:
            net_model = __import__('model_'+args.dim+'_'+afd, globals(), locals(), 0)
        log_path = make_model_path (net_model)
        datasets = load_datasets(path, afd, args.dim, args.syn, log_path, True)
        # train
        print("[INFO] Training model...")
        model.train_instances(datasets, net_model, log_path)
        print("[DONE] Finished.")

    else:
        print("[ERROR] Invalid action.")


if __name__ == '__main__':
    main()